{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd0cf61",
   "metadata": {},
   "source": [
    "# 02 | Neural Hydrology CARAVAN Setup\n",
    "\n",
    "## Purpose\n",
    "Configure and launch LSTM training on the CARAVAN dataset using globally snow-dominated basins.\n",
    "This notebook covers:\n",
    "1. Identifying snow-dominated basins from CARAVAN attributes\n",
    "2. Verifying the timeseries data format required by neuralhydrology\n",
    "3. Writing the neuralhydrology config (`.yml`)\n",
    "4. Writing and submitting the SLURM training job\n",
    "\n",
    "**Outputs produced by this notebook:**\n",
    "- `data/caravan_snow_basins.txt` — basin ID list for training/validation/test\n",
    "- `configs/caravan_snow_scenario1.yml` — neuralhydrology training config\n",
    "- `train_caravan_snow.slurm` — SLURM job script\n",
    "\n",
    "**Shared storage:**\n",
    "- CARAVAN data: `/uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data`\n",
    "- Project directory: `/uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project`\n",
    "\n",
    "**Note for new users:** \n",
    "> Any path containing `Meyer` should be updated to your own\n",
    "> project directory before running this notebook. The one path you will need to change\n",
    "> is `PROJECT_DIR` in Cell 3 — all other paths are derived from it automatically.\n",
    "> The shared CARAVAN data directory (`johnsonrc-group1`) does not need to change (assuming you are a UofU user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fffd2b",
   "metadata": {},
   "source": [
    "## About CARAVAN\n",
    "\n",
    "CARAVAN (Kratzert et al., 2023) is a global hydrology dataset designed specifically for large-sample,\n",
    "data-driven modeling. Key characteristics:\n",
    "\n",
    "| Property | Details |\n",
    "|---|---|\n",
    "| **Version used** | v1.6 |\n",
    "| **Total basins** | ~16,299 across 7 regions |\n",
    "| **Regions** | CAMELS (US), CAMELS-AUS, CAMELS-BR, CAMELS-CL, CAMELS-GB, HYSETS (Canada), LamaH (Europe) |\n",
    "| **Forcing data** | ERA5-Land reanalysis (1981–2020) |\n",
    "| **Basin attributes** | HydroATLAS catchment characteristics |\n",
    "| **Streamflow** | Observed daily discharge (mm/day, normalized by area) |\n",
    "| **Format** | One CSV per basin, neuralhydrology-native |\n",
    "\n",
    "### Why CARAVAN for Alaska transfer learning?\n",
    "- ERA5-Land forcing is globally consistent — the **same variables and units** we will extract for Alaska basins\n",
    "- Training on diverse global snow-dominated catchments exposes the model to a wide range of snow accumulation and melt regimes\n",
    "- neuralhydrology has built-in CARAVAN support (`dataset: caravan`), simplifying the data loading pipeline\n",
    "\n",
    "### Snow-dominated basin selection strategy\n",
    "We use the CARAVAN attribute `frac_snow` (fraction of precipitation falling as snow) with a threshold\n",
    "of **> 0.30**, consistent with our earlier CAMELS-US analysis. This yields ~3,297 basins globally,\n",
    "providing a rich and geographically diverse training set for snow-driven streamflow prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8f0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Paths configured\n",
      "  Project    : /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project\n",
      "  CARAVAN    : /uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Project paths ──────────────────────────────────────────────────────────────\n",
    "PROJECT_DIR      = Path(\"/uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project\")\n",
    "CARAVAN_DATA_DIR = Path(\"/uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data\")\n",
    "\n",
    "# ── Verify directories exist ───────────────────────────────────────────────────\n",
    "assert PROJECT_DIR.exists(),      f\"Project directory not found: {PROJECT_DIR}\"\n",
    "assert CARAVAN_DATA_DIR.exists(), f\"CARAVAN data not found: {CARAVAN_DATA_DIR}\"\n",
    "\n",
    "# ── Ensure output directories exist ───────────────────────────────────────────\n",
    "(PROJECT_DIR / \"data\").mkdir(exist_ok=True)\n",
    "(PROJECT_DIR / \"configs\").mkdir(exist_ok=True)\n",
    "(PROJECT_DIR / \"results\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"  Project    : {PROJECT_DIR}\")\n",
    "print(f\"  CARAVAN    : {CARAVAN_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f739a",
   "metadata": {},
   "source": [
    "## Step 1: Identify Snow-Dominated Basins\n",
    "\n",
    "CARAVAN attributes are organized by region, each with several thematic CSV files.\n",
    "The file we need is `attributes_caravan_{region}.csv`, which contains ERA5-Land derived\n",
    "climate indices including `frac_snow` — the long-term mean fraction of precipitation\n",
    "falling as snow.\n",
    "\n",
    "### Attribute file locations\n",
    "```\n",
    "CARAVAN_data/\n",
    "└── attributes/\n",
    "    ├── camels/\n",
    "    │   ├── attributes_caravan_camels.csv       ← contains frac_snow\n",
    "    │   ├── attributes_hydroatlas_camels.csv    ← basin geomorphology\n",
    "    │   └── attributes_other_camels.csv         ← gauge metadata, country, etc.\n",
    "    ├── camelsaus/\n",
    "    ├── camelsbr/\n",
    "    ├── camelscl/\n",
    "    ├── camelsgb/\n",
    "    ├── hysets/\n",
    "    └── lamah/\n",
    "```\n",
    "\n",
    "### Threshold selection\n",
    "We use `frac_snow > 0.30`, meaning at least 30% of annual precipitation falls as snow\n",
    "on average. This is the same threshold used in our CAMELS-US preliminary analysis and\n",
    "ensures the model sees basins where snow accumulation and melt are a primary driver\n",
    "of the hydrograph. Basins below this threshold are excluded from training — they would\n",
    "dilute the snow signal and are not representative of Alaska hydrology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2976b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering basins with frac_snow > 0.3\n",
      "============================================================\n",
      "  CAMELS      :  128 /   671 basins (19.1%)\n",
      "  CAMELSAUS   :    0 /   561 basins ( 0.0%)\n",
      "  CAMELSBR    :    0 /   870 basins ( 0.0%)\n",
      "  CAMELSCL    :  144 /   505 basins (28.5%)\n",
      "  CAMELSGB    :    0 /   671 basins ( 0.0%)\n",
      "  HYSETS      : 2803 / 12162 basins (23.0%)\n",
      "  LAMAH       :  222 /   859 basins (25.8%)\n",
      "============================================================\n",
      "  TOTAL       : 3297 snow-dominated basins\n",
      "\n",
      "frac_snow statistics across selected basins:\n",
      "  Mean   : 0.453\n",
      "  Median : 0.437\n",
      "  Min    : 0.300\n",
      "  Max    : 0.903\n",
      "\n",
      "✓ Basin list saved: /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project/data/caravan_snow_basins.txt\n",
      "  3,297 basin IDs written (format: region_gageID)\n",
      "  Example IDs: ['camels_01013500', 'camels_01022500', 'camels_01030500']\n"
     ]
    }
   ],
   "source": [
    "# ── Settings ───────────────────────────────────────────────────────────────────\n",
    "SNOW_THRESHOLD = 0.30\n",
    "REGIONS = ['camels', 'camelsaus', 'camelsbr', 'camelscl', 'camelsgb', 'hysets', 'lamah']\n",
    "attributes_dir = CARAVAN_DATA_DIR / 'attributes'\n",
    "\n",
    "# ── Filter snow-dominated basins by region ─────────────────────────────────────\n",
    "all_snow_basins = []\n",
    "\n",
    "print(f\"Filtering basins with frac_snow > {SNOW_THRESHOLD}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for region in REGIONS:\n",
    "    attr_file = attributes_dir / region / f\"attributes_caravan_{region}.csv\"\n",
    "    df = pd.read_csv(attr_file, index_col=0)\n",
    "\n",
    "    snow_basins = df[df['frac_snow'] > SNOW_THRESHOLD].copy()\n",
    "    snow_basins['region'] = region\n",
    "\n",
    "    pct = len(snow_basins) / len(df) * 100\n",
    "    print(f\"  {region.upper():12s}: {len(snow_basins):4d} / {len(df):5d} basins ({pct:4.1f}%)\")\n",
    "\n",
    "    all_snow_basins.append(snow_basins)\n",
    "\n",
    "# ── Combine all regions ────────────────────────────────────────────────────────\n",
    "snow_dominated = pd.concat(all_snow_basins)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"  {'TOTAL':12s}: {len(snow_dominated):4d} snow-dominated basins\\n\")\n",
    "\n",
    "print(\"frac_snow statistics across selected basins:\")\n",
    "print(f\"  Mean   : {snow_dominated['frac_snow'].mean():.3f}\")\n",
    "print(f\"  Median : {snow_dominated['frac_snow'].median():.3f}\")\n",
    "print(f\"  Min    : {snow_dominated['frac_snow'].min():.3f}\")\n",
    "print(f\"  Max    : {snow_dominated['frac_snow'].max():.3f}\")\n",
    "\n",
    "# ── Save basin ID list (plain text, one ID per line) ──────────────────────────\n",
    "basin_list_file = PROJECT_DIR / 'data' / 'caravan_snow_basins.txt'\n",
    "\n",
    "with open(basin_list_file, 'w') as f:\n",
    "    for basin_id in sorted(snow_dominated.index):\n",
    "        f.write(f\"{basin_id}\\n\")\n",
    "\n",
    "print(f\"\\n✓ Basin list saved: {basin_list_file}\")\n",
    "print(f\"  {len(snow_dominated):,} basin IDs written (format: region_gageID)\")\n",
    "print(f\"  Example IDs: {sorted(snow_dominated.index)[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08659a8",
   "metadata": {},
   "source": [
    "## Step 2: Verify CARAVAN Timeseries Format\n",
    "\n",
    "Before writing the neuralhydrology config, we need to confirm the exact column names\n",
    "and date format in the CARAVAN timeseries files. This is a one-time sanity check —\n",
    "neuralhydrology is strict about variable names and will throw an error at training time\n",
    "if any `dynamic_inputs` or `target_variables` are missing or misnamed.\n",
    "\n",
    "### Timeseries file locations\n",
    "```\n",
    "CARAVAN_data/\n",
    "└── timeseries/\n",
    "    └── csv/\n",
    "        ├── camels/\n",
    "        │   ├── camels_01013500.csv    ← one file per basin\n",
    "        │   ├── camels_01022500.csv\n",
    "        │   └── ...\n",
    "        ├── camelsaus/\n",
    "        ├── camelsbr/\n",
    "        ├── camelscl/\n",
    "        ├── camelsgb/\n",
    "        ├── hysets/\n",
    "        └── lamah/\n",
    "```\n",
    "\n",
    "Each file contains daily time steps as rows and ERA5-Land forcing variables + streamflow as columns.\n",
    "We will verify the following variables are present, as these are what we will pass to the model:\n",
    "\n",
    "| Role | Variable name |\n",
    "|---|---|\n",
    "| Forcing | `total_precipitation_sum` |\n",
    "| Forcing | `temperature_2m_max` |\n",
    "| Forcing | `temperature_2m_min` |\n",
    "| Forcing | `surface_net_solar_radiation_mean` |\n",
    "| Target | `streamflow` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43bd970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: camels_05495500.csv\n",
      "Shape      : (26662, 41)  (26662 days × 41 columns)\n",
      "\n",
      "Date column:\n",
      "  dtype  : object\n",
      "  format : 1951-01-01  (YYYY-MM-DD)\n",
      "  range  : 1951-01-01  →  2023-12-30\n",
      "\n",
      "All columns (41):\n",
      "  date\n",
      "  dewpoint_temperature_2m_max\n",
      "  dewpoint_temperature_2m_mean\n",
      "  dewpoint_temperature_2m_min\n",
      "  potential_evaporation_sum_ERA5_LAND\n",
      "  potential_evaporation_sum_FAO_PENMAN_MONTEITH\n",
      "  snow_depth_water_equivalent_max\n",
      "  snow_depth_water_equivalent_mean\n",
      "  snow_depth_water_equivalent_min\n",
      "  streamflow\n",
      "  surface_net_solar_radiation_max\n",
      "  surface_net_solar_radiation_mean\n",
      "  surface_net_solar_radiation_min\n",
      "  surface_net_thermal_radiation_max\n",
      "  surface_net_thermal_radiation_mean\n",
      "  surface_net_thermal_radiation_min\n",
      "  surface_pressure_max\n",
      "  surface_pressure_mean\n",
      "  surface_pressure_min\n",
      "  temperature_2m_max\n",
      "  temperature_2m_mean\n",
      "  temperature_2m_min\n",
      "  total_precipitation_sum\n",
      "  u_component_of_wind_10m_max\n",
      "  u_component_of_wind_10m_mean\n",
      "  u_component_of_wind_10m_min\n",
      "  v_component_of_wind_10m_max\n",
      "  v_component_of_wind_10m_mean\n",
      "  v_component_of_wind_10m_min\n",
      "  volumetric_soil_water_layer_1_max\n",
      "  volumetric_soil_water_layer_1_mean\n",
      "  volumetric_soil_water_layer_1_min\n",
      "  volumetric_soil_water_layer_2_max\n",
      "  volumetric_soil_water_layer_2_mean\n",
      "  volumetric_soil_water_layer_2_min\n",
      "  volumetric_soil_water_layer_3_max\n",
      "  volumetric_soil_water_layer_3_mean\n",
      "  volumetric_soil_water_layer_3_min\n",
      "  volumetric_soil_water_layer_4_max\n",
      "  volumetric_soil_water_layer_4_mean\n",
      "  volumetric_soil_water_layer_4_min\n",
      "\n",
      "============================================================\n",
      "Required variable check:\n",
      "============================================================\n",
      "  ✓  total_precipitation_sum\n",
      "  ✓  temperature_2m_max\n",
      "  ✓  temperature_2m_min\n",
      "  ✓  surface_net_solar_radiation_mean\n",
      "  ✓  streamflow  (target)\n",
      "\n",
      "✓ All required variables confirmed — safe to proceed with config\n"
     ]
    }
   ],
   "source": [
    "# ── Load a sample CAMELS basin timeseries ─────────────────────────────────────\n",
    "timeseries_dir = CARAVAN_DATA_DIR / 'timeseries' / 'csv'\n",
    "\n",
    "sample_file = next((timeseries_dir / 'camels').glob('*.csv'))\n",
    "df_sample = pd.read_csv(sample_file)\n",
    "\n",
    "print(f\"Sample file: {sample_file.name}\")\n",
    "print(f\"Shape      : {df_sample.shape}  ({df_sample.shape[0]} days × {df_sample.shape[1]} columns)\")\n",
    "print(f\"\\nDate column:\")\n",
    "print(f\"  dtype  : {df_sample['date'].dtype}\")\n",
    "print(f\"  format : {df_sample['date'].iloc[0]}  (YYYY-MM-DD)\")\n",
    "print(f\"  range  : {df_sample['date'].iloc[0]}  →  {df_sample['date'].iloc[-1]}\")\n",
    "\n",
    "print(f\"\\nAll columns ({len(df_sample.columns)}):\")\n",
    "for col in df_sample.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# ── Verify required variables are present ─────────────────────────────────────\n",
    "REQUIRED_INPUTS = [\n",
    "    'total_precipitation_sum',\n",
    "    'temperature_2m_max',\n",
    "    'temperature_2m_min',\n",
    "    'surface_net_solar_radiation_mean',\n",
    "]\n",
    "REQUIRED_TARGET = ['streamflow']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Required variable check:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "all_present = True\n",
    "for var in REQUIRED_INPUTS:\n",
    "    status = '✓' if var in df_sample.columns else '✗  MISSING'\n",
    "    print(f\"  {status}  {var}\")\n",
    "for var in REQUIRED_TARGET:\n",
    "    status = '✓' if var in df_sample.columns else '✗  MISSING'\n",
    "    print(f\"  {status}  {var}  (target)\")\n",
    "\n",
    "if all_present:\n",
    "    print(f\"\\n✓ All required variables confirmed — safe to proceed with config\")\n",
    "else:\n",
    "    print(f\"\\n✗ Missing variables detected — update dynamic_inputs before continuing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80123fba",
   "metadata": {},
   "source": [
    "## Step 3: Write the neuralhydrology Training Config\n",
    "\n",
    "The config is a `.yml` file that fully specifies a neuralhydrology experiment — data, model\n",
    "architecture, training hyperparameters, and logging. Every training run is reproducible from\n",
    "its config alone. Below is a section-by-section explanation of what we are setting and why.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment + paths\n",
    "- `experiment_name` — used as the run folder name prefix under `run_dir`\n",
    "- `run_dir` — where all outputs (weights, metrics, figures) are written\n",
    "- `train/validation/test_basin_file` — all three point to our 3,297-basin snow list;\n",
    "  neuralhydrology splits by **time**, not by basin, so the same basins appear in all three periods\n",
    "\n",
    "### Training / validation / test periods\n",
    "We use a classic hydrology split on the ERA5-Land record (1981–2020):\n",
    "\n",
    "| Split | Period | Purpose |\n",
    "|---|---|---|\n",
    "| Train | 1990–2010 | Model learning |\n",
    "| Validation | 2011–2015 | Epoch selection, early stopping |\n",
    "| Test | 2016–2020 | Final unbiased evaluation |\n",
    "\n",
    "We start in 1990 (not 1981) to avoid the ERA5-Land spin-up period and ensure\n",
    "soil moisture and snow states are well-initialized before training begins.\n",
    "\n",
    "### Dataset\n",
    "- `dataset: caravan` — activates neuralhydrology's built-in CARAVAN loader\n",
    "- `data_dir` — must point to the root `CARAVAN_data/` folder; the loader expects\n",
    "  the `timeseries/csv/{region}/` and `attributes/{region}/` subdirectory structure\n",
    "\n",
    "### Model architecture\n",
    "- `model: cudalstm` — GPU-optimized LSTM implementation; identical to `lstm` but faster\n",
    "- `hidden_size: 128` — number of LSTM memory cells; balances capacity vs. overfitting\n",
    "- `initial_forget_bias: 3` — initializes the forget gate to stay open, helping the LSTM\n",
    "  retain long-term snow accumulation signals early in training\n",
    "- `output_dropout: 0.4` — dropout applied to the output layer for regularization\n",
    "- `head: regression` + `output_activation: linear` — direct continuous streamflow prediction\n",
    "\n",
    "### Dynamic inputs + target\n",
    "- Four ERA5-Land variables chosen for physical relevance to snow-dominated hydrology:\n",
    "  precipitation, min/max temperature (drives melt), and solar radiation (energy balance)\n",
    "- `clip_targets_to_zero: [streamflow]` — prevents the model from predicting negative discharge\n",
    "\n",
    "### Optimizer + training\n",
    "- `optimizer: Adam` with a stepped learning rate schedule: 1e-3 → 5e-4 → 1e-4\n",
    "- `seq_length: 365` — one full year of antecedent conditions fed to the LSTM at each step,\n",
    "  critical for capturing the full snow accumulation season before spring melt\n",
    "- `predict_last_n: 1` — only the final timestep of each sequence is used for loss computation\n",
    "- `clip_gradient_norm: 1.0` — prevents exploding gradients during early training epochs\n",
    "\n",
    "### Validation + logging\n",
    "- `validate_every: 1` — compute validation metrics after every epoch\n",
    "- `validate_n_random_basins: 10` — evaluate on 10 random basins per epoch (fast proxy)\n",
    "- `save_weights_every: 1` — checkpoint saved each epoch so we can recover the best epoch\n",
    "- `save_validation_results: True` — writes per-basin metrics CSVs, required for best-epoch selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0af949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config written: /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project/configs/caravan_snow_scenario1.yml\n",
      "\n",
      "Key settings:\n",
      "  Experiment : caravan_snow_global_scenario1\n",
      "  Basins     : caravan_snow_basins.txt\n",
      "  data_dir   : /uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data\n",
      "  Epochs     : 30\n",
      "  seq_length : 365\n",
      "  hidden_size: 128\n"
     ]
    }
   ],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml_handler = YAML()\n",
    "yaml_handler.default_flow_style = False\n",
    "\n",
    "config = {\n",
    "    'experiment_name': 'caravan_snow_global_scenario1',\n",
    "    'run_dir': str(PROJECT_DIR / 'results' / 'caravan_snow_global_scenario1'),\n",
    "\n",
    "    'train_basin_file': str(basin_list_file),\n",
    "    'validation_basin_file': str(basin_list_file),\n",
    "    'test_basin_file': str(basin_list_file),\n",
    "\n",
    "    'train_start_date': '01/01/1990',\n",
    "    'train_end_date': '31/12/2010',\n",
    "    'validation_start_date': '01/01/2011',\n",
    "    'validation_end_date': '31/12/2015',\n",
    "    'test_start_date': '01/01/2016',\n",
    "    'test_end_date': '31/12/2020',\n",
    "\n",
    "    'dataset': 'caravan',\n",
    "    'data_dir': '/uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data',\n",
    "\n",
    "    'model': 'cudalstm',\n",
    "    'head': 'regression',\n",
    "    'output_activation': 'linear',\n",
    "    'hidden_size': 128,\n",
    "    'initial_forget_bias': 3,\n",
    "    'output_dropout': 0.4,\n",
    "\n",
    "    'dynamic_inputs': [\n",
    "        'total_precipitation_sum',\n",
    "        'temperature_2m_max',\n",
    "        'temperature_2m_min',\n",
    "        'surface_net_solar_radiation_mean',\n",
    "    ],\n",
    "    'target_variables': ['streamflow'],\n",
    "    'clip_targets_to_zero': ['streamflow'],\n",
    "\n",
    "    'optimizer': 'Adam',\n",
    "    'loss': 'MSE',\n",
    "    'epochs': 30,\n",
    "    'learning_rate': {0: 1e-3, 10: 5e-4, 20: 1e-4},\n",
    "    'batch_size': 256,\n",
    "    'clip_gradient_norm': 1.0,\n",
    "    'seq_length': 365,\n",
    "    'predict_last_n': 1,\n",
    "\n",
    "    'validate_every': 1,\n",
    "    'validate_n_random_basins': 10,\n",
    "    'cache_validation_data': True,\n",
    "    'metrics': ['NSE', 'MSE', 'KGE', 'Alpha-NSE', 'Beta-NSE'],\n",
    "\n",
    "    'device': 'cuda:0',\n",
    "    'num_workers': 8,\n",
    "    'seed': 42,\n",
    "\n",
    "    'log_interval': 50,\n",
    "    'log_tensorboard': True,\n",
    "    'log_n_figures': 5,\n",
    "    'save_weights_every': 1,\n",
    "    'save_validation_results': True,\n",
    "}\n",
    "\n",
    "config_file = PROJECT_DIR / 'configs' / 'caravan_snow_scenario1.yml'\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml_handler.dump(config, f)\n",
    "\n",
    "print(f\"✓ Config written: {config_file}\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Experiment : {config['experiment_name']}\")\n",
    "print(f\"  Basins     : {basin_list_file.name}\")\n",
    "print(f\"  data_dir   : {config['data_dir']}\")\n",
    "print(f\"  Epochs     : {config['epochs']}\")\n",
    "print(f\"  seq_length : {config['seq_length']}\")\n",
    "print(f\"  hidden_size: {config['hidden_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbafc2",
   "metadata": {},
   "source": [
    "## Step 4: Write the SLURM Training Script\n",
    "\n",
    "The SLURM script requests compute resources from CHPC's GRANITE cluster and launches\n",
    "the neuralhydrology training run. Below is a line-by-line explanation of each directive\n",
    "and command.\n",
    "\n",
    "---\n",
    "\n",
    "### SBATCH resource directives\n",
    "\n",
    "| Directive | Value | Explanation |\n",
    "|---|---|---|\n",
    "| `--account` | `rai` | Charge compute time to the RAI allocation |\n",
    "| `--partition` | `rai-gpu-grn` | GRANITE GPU nodes available to RAI |\n",
    "| `--qos` | `rai-gpu-grn` | Quality of service tier matching the partition |\n",
    "| `--nodes` | `1` | Single node — neuralhydrology data parallelism doesn't span nodes |\n",
    "| `--ntasks` | `1` | One training process |\n",
    "| `--cpus-per-task` | `16` | CPU cores for DataLoader workers (`num_workers: 8` needs headroom) |\n",
    "| `--mem` | `64G` | System RAM; CARAVAN loads basin data into memory during training |\n",
    "| `--gres=gpu` | `1` | One GPU; single-GPU training is stable and sufficient for 30 epochs |\n",
    "| `--time` | `48:00:00` | 48-hour wall time limit; our 30-epoch run took ~24 hours |\n",
    "| `--mail-type` | `BEGIN,END,FAIL` | Email notifications at job start, completion, or failure |\n",
    "\n",
    "---\n",
    "\n",
    "### Runtime commands\n",
    "\n",
    "**Module loading**\n",
    "- `module purge` — clears any inherited environment modules to ensure a clean state\n",
    "- `module load miniconda3/25.9.1` — loads the Conda installation available on GRANITE\n",
    "- `module load cuda/12.1` — loads CUDA toolkit matching our PyTorch build\n",
    "\n",
    "**Environment activation**\n",
    "- `source activate neuralhydrology` — activates the Conda environment where\n",
    "  neuralhydrology, PyTorch, and all dependencies are installed\n",
    "\n",
    "**Training launch**\n",
    "- `nh-run train --config-file configs/caravan_snow_scenario1.yml` — the neuralhydrology\n",
    "  CLI entry point; reads the config, initializes the model, and begins training.\n",
    "  All outputs (weights, metrics, figures, logs) are written to `run_dir` as specified in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9672756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SLURM script written: /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project/train_caravan_snow.slurm\n",
      "\n",
      "Resources requested:\n",
      "  Account   : rai\n",
      "  Partition : rai-gpu-grn\n",
      "  GPU       : 1x (GRANITE)\n",
      "  Memory    : 64 GB\n",
      "  Wall time : 48 hours\n",
      "\n",
      "To submit:\n",
      "  cd /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project\n",
      "  sbatch train_caravan_snow.slurm\n",
      "\n",
      "To monitor:\n",
      "  squeue -u $USER\n",
      "  tail -f results/slurm_<JOBID>.out\n"
     ]
    }
   ],
   "source": [
    "# ── Ensure results directory exists ───────────────────────────────────────────\n",
    "RESULTS_DIR = PROJECT_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "slurm_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --account=rai\n",
    "#SBATCH --partition=rai-gpu-grn\n",
    "#SBATCH --qos=rai-gpu-grn\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --time=48:00:00\n",
    "#SBATCH --job-name=caravan_snow_train\n",
    "#SBATCH --output=/uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project/results/slurm_%j.out\n",
    "#SBATCH --error=/uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project/results/slurm_%j.err\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=kaitlin.meyer@utah.edu\n",
    "\n",
    "# Print job info\n",
    "echo \"==========================================\"\n",
    "echo \"Job ID: $SLURM_JOB_ID\"\n",
    "echo \"Node: $SLURM_NODELIST\"\n",
    "echo \"Partition: $SLURM_JOB_PARTITION\"\n",
    "echo \"QOS: $SLURM_JOB_QOS\"\n",
    "echo \"Account: $SLURM_JOB_ACCOUNT\"\n",
    "echo \"Start time: $(date)\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "# Load modules\n",
    "module purge\n",
    "module load miniconda3/25.9.1\n",
    "module load cuda/12.1\n",
    "\n",
    "# Activate conda environment\n",
    "source activate neuralhydrology\n",
    "\n",
    "# Print environment info\n",
    "echo \"Python: $(which python)\"\n",
    "echo \"CUDA available: $(python -c 'import torch; print(torch.cuda.is_available())')\"\n",
    "echo \"GPU: $(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader)\"\n",
    "\n",
    "# Run training\n",
    "cd /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project\n",
    "echo \"Starting training at $(date)\"\n",
    "nh-run train --config-file configs/caravan_snow_scenario1.yml\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Job completed at: $(date)\"\n",
    "echo \"==========================================\"\n",
    "\"\"\"\n",
    "\n",
    "# ── Write SLURM script ─────────────────────────────────────────────────────────\n",
    "slurm_file = PROJECT_DIR / \"train_caravan_snow.slurm\"\n",
    "with open(slurm_file, 'w') as f:\n",
    "    f.write(slurm_script)\n",
    "\n",
    "print(f\"✓ SLURM script written: {slurm_file}\")\n",
    "print(f\"\\nResources requested:\")\n",
    "print(f\"  Account   : rai\")\n",
    "print(f\"  Partition : rai-gpu-grn\")\n",
    "print(f\"  GPU       : 1x (GRANITE)\")\n",
    "print(f\"  Memory    : 64 GB\")\n",
    "print(f\"  Wall time : 48 hours\")\n",
    "print(f\"\\nTo submit:\")\n",
    "print(f\"  cd {PROJECT_DIR}\")\n",
    "print(f\"  sbatch train_caravan_snow.slurm\")\n",
    "print(f\"\\nTo monitor:\")\n",
    "print(f\"  squeue -u $USER\")\n",
    "print(f\"  tail -f results/slurm_<JOBID>.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f3d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Submission Checklist:\n",
      "============================================================\n",
      "  ✓  CARAVAN data directory exists\n",
      "  ✓  Basin list file exists\n",
      "  ✓  Config file exists\n",
      "  ✓  SLURM script exists\n",
      "  ✓  Results directory exists\n",
      "\n",
      "  Basin list contains 3,297 basins\n",
      "\n",
      "Config spot-check:\n",
      "  experiment_name : caravan_snow_global_scenario1\n",
      "  data_dir        : /uufs/chpc.utah.edu/common/home/johnsonrc-group1/CARAVAN/CARAVAN_data\n",
      "  train period    : 01/01/1990 → 31/12/2010\n",
      "  epochs          : 30\n",
      "  seq_length      : 365\n",
      "  dynamic_inputs  : ['total_precipitation_sum', 'temperature_2m_max', 'temperature_2m_min', 'surface_net_solar_radiation_mean']\n",
      "  target          : ['streamflow']\n",
      "\n",
      "============================================================\n",
      "✓ All checks passed — ready to submit\n",
      "============================================================\n",
      "\n",
      "Submit with:\n",
      "  cd /uufs/chpc.utah.edu/common/home/civil-group1/Meyer/neuralhydrology_project\n",
      "  sbatch train_caravan_snow.slurm\n"
     ]
    }
   ],
   "source": [
    "# ── Pre-submission checklist ───────────────────────────────────────────────────\n",
    "print(\"Pre-Submission Checklist:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = {\n",
    "    'CARAVAN data directory exists'  : CARAVAN_DATA_DIR.exists(),\n",
    "    'Basin list file exists'         : basin_list_file.exists(),\n",
    "    'Config file exists'             : config_file.exists(),\n",
    "    'SLURM script exists'            : slurm_file.exists(),\n",
    "    'Results directory exists'       : RESULTS_DIR.exists(),\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "for description, status in checks.items():\n",
    "    symbol = '✓' if status else '✗'\n",
    "    print(f\"  {symbol}  {description}\")\n",
    "    if not status:\n",
    "        all_good = False\n",
    "\n",
    "# ── Basin count verification ───────────────────────────────────────────────────\n",
    "print()\n",
    "with open(basin_list_file, 'r') as f:\n",
    "    n_basins = sum(1 for line in f if line.strip())\n",
    "print(f\"  Basin list contains {n_basins:,} basins\")\n",
    "\n",
    "# ── Config spot-check ──────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"Config spot-check:\")\n",
    "print(f\"  experiment_name : {config['experiment_name']}\")\n",
    "print(f\"  data_dir        : {config['data_dir']}\")\n",
    "print(f\"  train period    : {config['train_start_date']} → {config['train_end_date']}\")\n",
    "print(f\"  epochs          : {config['epochs']}\")\n",
    "print(f\"  seq_length      : {config['seq_length']}\")\n",
    "print(f\"  dynamic_inputs  : {config['dynamic_inputs']}\")\n",
    "print(f\"  target          : {config['target_variables']}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "if all_good:\n",
    "    print(\"✓ All checks passed — ready to submit\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nSubmit with:\")\n",
    "    print(f\"  cd {PROJECT_DIR}\")\n",
    "    print(f\"  sbatch train_caravan_snow.slurm\")\n",
    "else:\n",
    "    print(\"✗ Some checks failed — resolve before submitting\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
